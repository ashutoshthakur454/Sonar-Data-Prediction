{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b59fda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b7a4fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Files/Sonar_data.csv\",header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "583eda78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7462485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "56    0\n",
       "57    0\n",
       "58    0\n",
       "59    0\n",
       "60    0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8cc0e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9   ...          50  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \n",
       "std      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \n",
       "min      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n",
       "25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n",
       "50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n",
       "75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \n",
       "max      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n",
       "\n",
       "               51          52          53          54          55          56  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \n",
       "std      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \n",
       "min      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n",
       "25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n",
       "50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n",
       "75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \n",
       "max      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n",
       "\n",
       "               57          58          59  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.007949    0.007941    0.006507  \n",
       "std      0.006470    0.006181    0.005031  \n",
       "min      0.000300    0.000100    0.000600  \n",
       "25%      0.003600    0.003675    0.003100  \n",
       "50%      0.005800    0.006400    0.005300  \n",
       "75%      0.010350    0.010325    0.008525  \n",
       "max      0.044000    0.036400    0.043900  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c98a3ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "            51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "225df303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    111\n",
       "R     97\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[60].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38c312d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.034989</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>0.050720</td>\n",
       "      <td>0.064768</td>\n",
       "      <td>0.086715</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>0.128359</td>\n",
       "      <td>0.149832</td>\n",
       "      <td>0.213492</td>\n",
       "      <td>0.251022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019352</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.011643</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.009923</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>0.006930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.022498</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.035951</td>\n",
       "      <td>0.041447</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.096224</td>\n",
       "      <td>0.114180</td>\n",
       "      <td>0.117596</td>\n",
       "      <td>0.137392</td>\n",
       "      <td>0.159325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012311</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.006024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "60                                                                         \n",
       "M   0.034989  0.045544  0.050720  0.064768  0.086715  0.111864  0.128359   \n",
       "R   0.022498  0.030303  0.035951  0.041447  0.062028  0.096224  0.114180   \n",
       "\n",
       "          7         8         9   ...        50        51        52        53  \\\n",
       "60                                ...                                           \n",
       "M   0.149832  0.213492  0.251022  ...  0.019352  0.016014  0.011643  0.012185   \n",
       "R   0.117596  0.137392  0.159325  ...  0.012311  0.010453  0.009640  0.009518   \n",
       "\n",
       "          54        55        56        57        58        59  \n",
       "60                                                              \n",
       "M   0.009923  0.008914  0.007825  0.009060  0.008695  0.006930  \n",
       "R   0.008567  0.007430  0.007814  0.006677  0.007078  0.006024  \n",
       "\n",
       "[2 rows x 60 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(60).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09f2cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(60,axis=1)\n",
    "Y=df[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fb11b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         0       1       2       3       4       5       6       7       8   \\\n",
       " 0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       " 1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       " 2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       " 3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       " 4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       " ..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       " 203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       " 204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       " 205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       " 206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       " 207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       " \n",
       "          9   ...      50      51      52      53      54      55      56  \\\n",
       " 0    0.2111  ...  0.0232  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180   \n",
       " 1    0.2872  ...  0.0125  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140   \n",
       " 2    0.6194  ...  0.0033  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316   \n",
       " 3    0.1264  ...  0.0241  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050   \n",
       " 4    0.4459  ...  0.0156  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072   \n",
       " ..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       " 203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n",
       " 204  0.2154  ...  0.0051  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034   \n",
       " 205  0.2529  ...  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140   \n",
       " 206  0.2354  ...  0.0042  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034   \n",
       " 207  0.2354  ...  0.0181  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040   \n",
       " \n",
       "          57      58      59  \n",
       " 0    0.0084  0.0090  0.0032  \n",
       " 1    0.0049  0.0052  0.0044  \n",
       " 2    0.0164  0.0095  0.0078  \n",
       " 3    0.0044  0.0040  0.0117  \n",
       " 4    0.0048  0.0107  0.0094  \n",
       " ..      ...     ...     ...  \n",
       " 203  0.0115  0.0193  0.0157  \n",
       " 204  0.0032  0.0062  0.0067  \n",
       " 205  0.0138  0.0077  0.0031  \n",
       " 206  0.0079  0.0036  0.0048  \n",
       " 207  0.0036  0.0061  0.0115  \n",
       " \n",
       " [208 rows x 60 columns],\n",
       " 0      R\n",
       " 1      R\n",
       " 2      R\n",
       " 3      R\n",
       " 4      R\n",
       "       ..\n",
       " 203    M\n",
       " 204    M\n",
       " 205    M\n",
       " 206    M\n",
       " 207    M\n",
       " Name: 60, Length: 208, dtype: object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdc8fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "203    1\n",
      "204    1\n",
      "205    1\n",
      "206    1\n",
      "207    1\n",
      "Name: 60, Length: 208, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "Y=((Y=='M').astype(int))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "016b88de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 60) (32, 60) (176,) (32,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, stratify=Y, random_state=1)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f00e03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "757cdae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Logistic Regression: Train accuracy: 0.8181818181818182  test accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "train_data_accuracy=model.score(X_train,Y_train)\n",
    "test_data_accuracy=model.score(X_test,Y_test)\n",
    "print(\"For Logistic Regression:\",\"Train accuracy:\",train_data_accuracy,\" test accuracy:\",test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27aecef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Poly of deg 1 SVC: Train accuracy: 0.7897727272727273  test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model=svm.SVC(kernel='poly',degree=1)\n",
    "model.fit(X_train,Y_train)\n",
    "train_data_accuracy=model.score(X_train,Y_train)\n",
    "test_data_accuracy=model.score(X_test,Y_test)\n",
    "print(\"For Poly of deg 1 SVC:\",\"Train accuracy:\",train_data_accuracy,\" test accuracy:\",test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9af7e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Linear SVC: Train accuracy: 0.8579545454545454  test accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "model=svm.SVC(kernel='linear')\n",
    "model.fit(X_train,Y_train)\n",
    "train_data_accuracy=model.score(X_train,Y_train)\n",
    "test_data_accuracy=model.score(X_test,Y_test)\n",
    "print(\"For Linear SVC:\",\"Train accuracy:\",train_data_accuracy,\" test accuracy:\",test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6377504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a47afb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 1ms/step - loss: 0.6975 - accuracy: 0.4432\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6806 - accuracy: 0.5511\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.6250\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6317 - accuracy: 0.7159\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5897 - accuracy: 0.7443\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5340 - accuracy: 0.7955\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8239\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8239\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8580\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3854 - accuracy: 0.8125\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3583 - accuracy: 0.8352\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8693\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8523\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8693\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8977\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.9205\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9091\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2085 - accuracy: 0.9261\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2273 - accuracy: 0.9261\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9261\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9261\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9148\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9261\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.8807\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9773\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9148\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1596 - accuracy: 0.9375\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9489\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9489\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9773\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9943\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9886\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9943\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0501 - accuracy: 0.9943\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9886\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0473 - accuracy: 0.9943\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9886\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9943\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0447 - accuracy: 0.9830\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 968us/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 965us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 970us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 887us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 874us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 864us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 871us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 947us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 907us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 838us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 987us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 927us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 990us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 857us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 871us/step - loss: 9.8161e-04 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 9.1888e-04 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.4555e-04 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 8.1540e-04 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 7.6736e-04 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 810us/step - loss: 7.2930e-04 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 837us/step - loss: 6.6978e-04 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 926us/step - loss: 6.4967e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 926us/step - loss: 6.0680e-04 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 939us/step - loss: 5.8822e-04 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 894us/step - loss: 5.6014e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 912us/step - loss: 5.2570e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 5.0301e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 784us/step - loss: 4.7643e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 944us/step - loss: 4.5202e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 921us/step - loss: 4.3255e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 898us/step - loss: 4.1541e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 861us/step - loss: 3.9346e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 876us/step - loss: 3.9487e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 815us/step - loss: 3.7150e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 843us/step - loss: 3.5606e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 915us/step - loss: 3.3730e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 992us/step - loss: 3.2223e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 869us/step - loss: 3.1301e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 846us/step - loss: 2.9821e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 867us/step - loss: 2.8512e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 931us/step - loss: 2.7593e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 835us/step - loss: 2.6872e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 767us/step - loss: 2.5799e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 957us/step - loss: 2.5067e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 886us/step - loss: 2.3884e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 796us/step - loss: 2.3253e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 960us/step - loss: 2.2278e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 819us/step - loss: 2.1487e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x269a98b9220>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
    "    keras.layers.Dense(45, activation='relu'),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60aad3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3076 - accuracy: 0.7188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3075578212738037, 0.71875]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e41f17b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).reshape(-1)\n",
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ffd32b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69        15\n",
      "           1       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.72        32\n",
      "   macro avg       0.72      0.72      0.72        32\n",
      "weighted avg       0.72      0.72      0.72        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64eff28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 1ms/step - loss: 0.6999 - accuracy: 0.5057\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6907 - accuracy: 0.5398\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.4773\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.5966\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6863 - accuracy: 0.5284\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5170\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6808 - accuracy: 0.5511\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6764 - accuracy: 0.5966\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6892 - accuracy: 0.5341\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6737 - accuracy: 0.5852\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6633 - accuracy: 0.5909\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6681 - accuracy: 0.6080\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6761\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.6818\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6622 - accuracy: 0.5966\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.6818\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.7273\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.6761\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.7159\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7557\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7443\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5933 - accuracy: 0.6932\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5458 - accuracy: 0.7727\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7670\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.7500\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7955\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5087 - accuracy: 0.7727\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4743 - accuracy: 0.8068\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8239\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7898\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7898\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4492 - accuracy: 0.7898\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4443 - accuracy: 0.8068\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.8182\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8523\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.8068\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8580\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8409\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8693\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8580\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8523\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8295\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8750\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8409\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4105 - accuracy: 0.8011\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2948 - accuracy: 0.9148\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.9148\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8920\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2839 - accuracy: 0.9261\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8750\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2861 - accuracy: 0.8864\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8750\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 908us/step - loss: 0.3117 - accuracy: 0.8580\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 941us/step - loss: 0.2771 - accuracy: 0.9148\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2891 - accuracy: 0.8807\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 941us/step - loss: 0.3166 - accuracy: 0.8636\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.2977 - accuracy: 0.8864\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 923us/step - loss: 0.3034 - accuracy: 0.8920\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.9261\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 923us/step - loss: 0.2186 - accuracy: 0.9261\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 989us/step - loss: 0.2259 - accuracy: 0.9034\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 908us/step - loss: 0.2135 - accuracy: 0.9318\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 984us/step - loss: 0.2541 - accuracy: 0.9034\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2340 - accuracy: 0.9034\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.8977\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 988us/step - loss: 0.2496 - accuracy: 0.9091\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9261\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9261\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 951us/step - loss: 0.1906 - accuracy: 0.9261\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 904us/step - loss: 0.2222 - accuracy: 0.9261\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 934us/step - loss: 0.1904 - accuracy: 0.9375\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 872us/step - loss: 0.1847 - accuracy: 0.9432\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 832us/step - loss: 0.2059 - accuracy: 0.9432\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 892us/step - loss: 0.2318 - accuracy: 0.9261\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 858us/step - loss: 0.1691 - accuracy: 0.9432\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 841us/step - loss: 0.1983 - accuracy: 0.9261\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 908us/step - loss: 0.1927 - accuracy: 0.9375\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 894us/step - loss: 0.2478 - accuracy: 0.9148\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 842us/step - loss: 0.1521 - accuracy: 0.9432\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 826us/step - loss: 0.1638 - accuracy: 0.9432\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 859us/step - loss: 0.2156 - accuracy: 0.9205\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 898us/step - loss: 0.1959 - accuracy: 0.9432\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 877us/step - loss: 0.1479 - accuracy: 0.9659\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 850us/step - loss: 0.1488 - accuracy: 0.9545\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 865us/step - loss: 0.1840 - accuracy: 0.9205\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 884us/step - loss: 0.1392 - accuracy: 0.9602\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 879us/step - loss: 0.1427 - accuracy: 0.9602\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 928us/step - loss: 0.1477 - accuracy: 0.9489\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 853us/step - loss: 0.1368 - accuracy: 0.9545\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 869us/step - loss: 0.1593 - accuracy: 0.9489\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.9375\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9545\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9375\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1524 - accuracy: 0.9545\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9489\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9489\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9602\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x269a99e26a0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld = keras.Sequential([\n",
    "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(45, activation='relu'),\n",
    "    keras.layers.Dropout(0.35),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "modeld.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modeld.fit(X_train, Y_train, epochs=100, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc507f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2511 - accuracy: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2511020302772522, 0.90625]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50672946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = modeld.predict(X_test).reshape(-1)\n",
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb8d6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        15\n",
      "           1       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.91        32\n",
      "   macro avg       0.91      0.90      0.91        32\n",
      "weighted avg       0.91      0.91      0.91        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa795b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
